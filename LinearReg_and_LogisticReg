import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# This function is to load the training dataset and testing dataset (from the given .csv files)
# train_path -- the paths of training dataset,
# test_path -- the paths testing dataset,
# X_train --  normalized training attribute value array,
# y_train -- normalized training label vector, 
# X_test -- normalized test attribute value array,
# y_test -- normalized tes label vector,
# label - labels of each variable
def load_dataset(train_path,test_path):
    
    train_data = pd.read_csv(train_path, sep=',').values
    test_data = pd.read_csv(test_path, sep=',').values
    label = pd.read_csv(train_path, sep=',').columns

    train_rows = train_data.shape[0]
    test_rows = test_data.shape[0]

    norm_train_array, norm_test_array = scaleFeature(train_data, test_data)

    # append a column of ones to the front of the datasets as x_0
    train_append_ones = np.ones([train_rows,1])
    test_append_ones = np.ones([test_rows,1])
    loaded_train_array = np.hstack((train_append_ones,norm_train_array))
    loaded_test_array = np.hstack((test_append_ones,norm_test_array))
    train_columns = loaded_train_array.shape[1]                      
    test_columns = loaded_test_array.shape[1]

    X_train = loaded_train_array[:,:-1]
    y_train = loaded_train_array[:,-1:]
    X_test = loaded_test_array[:,:-1]
    y_test = loaded_test_array[:,-1:]
   
    return X_train, y_train, X_test, y_test, label

# This function is to create a vector of zeros of shape (dim, 1) for theta.
# dim -- the size of the theta vector we want (or the number of parameters in this case),
# theta -- initialized vector of shape (dim, 1).

def initialize_with_zeros(dim):
    theta = np.zeros([dim,1])
    return theta

# This is a proprocessing function to normalize original data to [0,1].
# train_array -- unnormalized training array,
# test_array -- unnormalized test array,
# norm_train_array -- normalized training array,
# norm_test_array -- normalized test array.
def scaleFeature(train_array, test_array):
    
    train_rows = train_array.shape[0]
    test_rows = test_array.shape[0]
    array_min = train_array.min(0)
    array_range = train_array.max(0) - train_array.min(0)
    norm_train_array = (train_array - array_min) / array_range
    norm_test_array = (test_array - test_array.min(0)) /  (test_array.max(0) - test_array.min(0))
    return norm_train_array, norm_test_array

# This function is to compute the sigmoid of z (z is a scalar or numpy array of any size.), s is the result.
def sigmoid(z):
    s = 1/(1+np.exp(-z))
    return s

# This function is to calculate the cost between predicted values and label values by different cost functions.
# theta -- weights, a numpy array,
# X -- attribute data,
# y -- true "label" vector,
# r_type -- indicator, a string ('linear' or 'logistic'),
# cost -- calculated cost, a number. 
def computeCost(theta, X, y, r_type):

    m = len(y)

    # linear regression part
    if r_type == 'linear':
        # se -- sum of square of error
        cal1 = y - np.dot(X,theta)
        se = np.dot(np.transpose(cal1),cal1)
        cost = np.float(1/(2*m)*se)
    # logistic regression part 
    elif r_type == 'logistic':
        # A -- value computed by the sigmoid function
        A = sigmoid(np.dot(X,theta))        
        cost = -1/m * np.sum(y*np.log(A) + (1-y)*np.log(1-A),axis = 0)
    else:
        print ('No such regression type.(The expected r_type should be linear or logistic.)')
    
    return cost
   


# This function is to optimizes theta and b by running a gradient descent algorithm.
# theta -- weights, a numpy array,
# X -- attribute data,
# y -- true "label" vector,
# learning_rate -- learning rate of the gradient descent update rule,
# num_iterations -- number of iterations of the optimization loop,
# r_type -- indicator, a string ('linear' or 'logistic'),
# theta -- learnt weights, a numpy array,
# cost_list -- list of all the costs computed during the optimization, this will be used to plot the lear.
def gradientDescent(theta, X, y, learning_rate, num_iterations, r_type):
   
    r_type = r_type
    cost_list = []
    m = len(y)
    file = open('costByIteration_'+ r_type + '.txt','a')
    
    for i in range(num_iterations):
        if r_type == 'linear':
            y_current = np.dot(X,theta)
            d_theta = 1/m * np.dot(np.transpose(X),(y_current - y))
            theta = theta - learning_rate*d_theta
            cost = computeCost(theta, X, y, r_type)
            cost_list.append(cost)
            s = 'Iteration '+str(i+1)+': '+str(cost)
            if (i+1) % 100 == 0:
                print(s)
            file.write(s + '\n')

            
        elif r_type == 'logistic':
            A = sigmoid(np.dot(X,theta))  
            d_theta = 1/m * np.dot(np.transpose(X),(A - y))
            theta = theta - learning_rate*d_theta
            cost = computeCost(theta, X, y, r_type)
            cost_list.append(cost)
            s = 'Iteration '+str(i+1)+': '+str(cost)
            if (i+1) % 100 == 0:
                print(s)
            file.write(s + '\n')
        
        else:
            print ('No such regression type.(The expected r_type should be linear or logistic.)')
    
    return theta, cost_list


# This function is to predict whether the label is 0 or 1 using learned logistic regression parameters (theta, b)
# theta -- weights, a numpy array,
# X -- attribute data,
# y_prediction -- a numpy array (vector) containing all predictions for the examples in X.
def predict(theta, X):
    
    m = X.shape[0]
    y_prediction = np.zeros((m,1))
  
    A = sigmoid(np.dot(X, theta))
   
    for i in range(A.shape[0]):
        
        if A[i,0] <= 0.5:
            y_prediction[i][0] = 0
        else:
            y_prediction[i][0] = 1
    
    return y_prediction


# This function is to build a linear regression model.
# train_file -- file name of training set
# test_file -- file name of test set 
# num_iterations -- number of iterations of the optimization loop,
# learning_rate -- learning rate of the gradient descent update rule,
# r_type -- indicator, a string ('linear' or 'logistic').
# Draw a cost curve named 'cost_curve_linear.png'.
# Save all cost result in 'costByInteration_linear.png'
def linear_model(train_file, test_file, num_iterations, learning_rate, r_type='linear'):
    r_type = r_type
    # load the dataset
    X_train, y_train, X_test, y_test, label = load_dataset(os.getcwd() + '/' + train_file, os.getcwd() + '/' + test_file)
    # print the general informations of the dataset
    print ("\n **** Linear Regression **** \n")
    print ("Number of training examples: train_rows = " + str(X_train.shape[0]))
    print ("Number of testing examples: test_rows = " + str(X_test.shape[0]))
    print ("train_set_x shape: " + str(X_train.shape))
    print ("train_set_y shape: " + str(y_train.shape))
    print ("test_set_x shape: " + str(X_test.shape))
    print ("test_set_y shape: " + str(y_test.shape))
    print ("Features: " + str(label.tolist()))

    theta = initialize_with_zeros(X_train.shape[1])
    theta,  cost_train = gradientDescent(theta, X_train, y_train, learning_rate, num_iterations, r_type)

    # calculate the test cost
    cost_test = computeCost(theta, X_test, y_test, r_type)

    # draw the training cost curve
    fig = plt.figure()
    plt.xlabel('Iteration')
    plt.ylabel('Cost')
    plt.plot(range(num_iterations),cost_train,'y-',figure=fig)
    #plt.show()
    plt.savefig('./cost_curve_' + r_type + '.png')
    print("Costs saved: 'cost_curve_%s.png, costByIteration_%s.txt'."%(r_type,r_type))
    plt.close()
     
    # print the final coefficient vector, training cost, test cost, training accuracy and test accuracy
    for i in range(len(label.tolist())-1):
        print('theta('+str(label.tolist()[i])+ ') = '+str(theta[i,0]))
    # print('Label: '+str(label.tolist()))
    # print('Theta:')
    # print(params["theta"])

    print('Training Cost = ', cost_train[-1])
    print('Testing Cost = ', cost_test)
    print("Training accuracy: {} %".format(100 - cost_train[-1] * 100))
    print("Testing accuracy: {} %".format(100 - cost_test * 100))

    return 



# This function is to build a logistic regression model
# train_file -- file name of training set
# test_file -- file name of test set 
# num_iterations -- number of iterations of the optimization loop,
# learning_rate -- learning rate of the gradient descent update rule,
# r_type -- indicator, a string ('linear' or 'logistic').
# Draw a cost curve named 'cost_curve_logistic.png'.
# Save all cost result in 'costByInteration_logistic.png'
def logistic_model(train_file, test_file, num_iterations, learning_rate, r_type='logistic'):
    r_type = r_type
    # load the dataset
    X_train, y_train, X_test, y_test, label = load_dataset(os.getcwd() + '/' + train_file, os.getcwd() + '/' + test_file)
    # print the general informations of the dataset
    print ("\n **** Logistic Regression **** \n")
    print ("Number of training examples: train_rows = " + str(X_train.shape[0]))
    print ("Number of testing examples: test_rows = " + str(X_test.shape[0]))
    print ("train_set_x shape: " + str(X_train.shape))
    print ("train_set_y shape: " + str(y_train.shape))
    print ("test_set_x shape: " + str(X_test.shape))
    print ("test_set_y shape: " + str(y_test.shape))
    print ("Features: " + str(label.tolist()))

    theta = initialize_with_zeros(X_train.shape[1])
    theta, cost_train = gradientDescent(theta, X_train, y_train, learning_rate, num_iterations, r_type)

    # calculate the test cost
    cost_test = computeCost(theta, X_test, y_test, r_type)

    # draw the training cost curve
    fig = plt.figure()
    plt.xlabel('Iteration')
    plt.ylabel('Cost')
    plt.plot(range(num_iterations),cost_train,'y-',figure=fig)
    #plt.show()
    plt.savefig('./cost_curve_' + r_type + '.png')
    print("Costs saved: 'cost_curve_%s.png, costByIteration_%s.txt'."%(r_type,r_type))
    plt.close()
     
    # print the final coefficient vector, training cost, test cost, training accuracy and test accuracy
    for i in range(len(label.tolist())-1):
        print('theta('+str(label.tolist()[i])+ ') = '+str(theta[i,0]))

    y_prediction_train = predict(theta, X_train)
    y_prediction_test = predict(theta, X_test)
    print("train accuracy: {} %".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))
    print("test accuracy: {} %".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))
    
    return 









